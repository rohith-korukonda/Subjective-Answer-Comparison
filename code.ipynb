{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group_43.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing necessary libraries and dependencies"
      ],
      "metadata": {
        "id": "iWu96YpHlsDn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z2WAytZBZ_q",
        "outputId": "33b1ccfa-2f7e-4804-fdaf-5ec3c40e7ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import json\n",
        "import gensim.downloader as api\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing"
      ],
      "metadata": {
        "id": "x89ZaX-Ulzbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contraction = {\n",
        "    \"ain't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n",
        "    \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
        "    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n",
        "    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
        "    \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
        "    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
        "    \"this's\": \"this is\",\n",
        "    \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
        "    \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
        "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
        "    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "    \n",
        "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
        "stopw = set(stopwords.words('english'))\n",
        "\n",
        "def clean(text):\n",
        "    text = text.lower()\n",
        "    temp = \"\"\n",
        "    for i in text.split():\n",
        "        try:\n",
        "            temp+=contraction[i]+' '\n",
        "        except:\n",
        "            temp+= i+' '\n",
        "    text = temp.strip()\n",
        "    text = text.lower().translate(remove_punctuation_map)\n",
        "    text = re.sub(\"[^a-zA-Z#]\",\" \",text)\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\",\", \"\", text)\n",
        "    text = re.sub(r\"\\.\", \"\", text)\n",
        "    text = re.sub(r\"!\", \"!\", text)\n",
        "    text = re.sub(r\"\\/\", \"\", text)\n",
        "    text = re.sub(r\"'\", \"\", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \":\", text)\n",
        "    text = re.sub(r' +',' ',text)\n",
        "    return text.strip()\n",
        "\n",
        "def stopwordremoval(text):\n",
        "    text = word_tokenize(text)\n",
        "    text = [i for i in text if i not in stopw]\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "Vhn89YAzBmEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First test case preparation (Test case 1)"
      ],
      "metadata": {
        "id": "3v67BJsbl4G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Machine learning is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult to develop a conventional algorithm for effectively performing the task.\"\"\"\n",
        "alt = \"\"\" Machine learning is the  study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult to develop a conventional algorithm for effectively performing the task.\"\"\"\n",
        "text2 = word_tokenize(text.lower().translate(remove_punctuation_map))\n",
        "text2 = [i for i in text2 if i not in stopw]\n",
        "alt2 = word_tokenize(alt.lower().translate(remove_punctuation_map))\n",
        "alt2 = [i for i in alt2 if i not in stopw]"
      ],
      "metadata": {
        "id": "X8b0F7koCKIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building"
      ],
      "metadata": {
        "id": "9e34d6msmEUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim, smart_open\n",
        "\n",
        "def read_corpus(fname, tokens_only=False):\n",
        "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            tokens = gensim.utils.simple_preprocess(line)\n",
        "            print(line)\n",
        "            if tokens_only:\n",
        "                yield tokens\n",
        "            else:\n",
        "                # For training data, add tags\n",
        "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
        "                \n",
        "def read_corpus_semeval(tokens_only=False):\n",
        "    i = 0\n",
        "    doc = api.load(\"semeval-2016-2017-task3-subtaskA-unannotated\")\n",
        "    for dictionary in doc:\n",
        "        sentList = []\n",
        "        for com in dictionary[\"RelComments\"]:\n",
        "            sentList.append(word_tokenize(clean(com[\"RelCText\"])))\n",
        "        sentList.append(word_tokenize(clean(dictionary[\"RelQuestion\"][\"RelQBody\"])))\n",
        "        for sent in sentList:\n",
        "            if tokens_only:\n",
        "                yield sent\n",
        "            else:\n",
        "                # For training data, add tags\n",
        "                yield gensim.models.doc2vec.TaggedDocument(sent, [i])\n",
        "                i += 1\n",
        "\n",
        "def read_fakenews(tokens_only=False):\n",
        "    doc = api.load(\"fake-news\")\n",
        "    i = 0\n",
        "    for line in doc: \n",
        "        dictionary = eval(json.dumps(line))\n",
        "        q = word_tokenize(clean(dictionary[\"title\"]))\n",
        "        t = [word_tokenize(clean(i)) for i in sent_tokenize(dictionary[\"text\"])]\n",
        "        t.append(q)\n",
        "        for sent in t:\n",
        "            if tokens_only:\n",
        "                yield sent\n",
        "            else:\n",
        "                # For training data, add tags\n",
        "                yield gensim.models.doc2vec.TaggedDocument(sent, [i])\n",
        "                i += 1"
      ],
      "metadata": {
        "id": "yQQisEXTCcLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc1 = read_corpus_semeval()"
      ],
      "metadata": {
        "id": "IyeL33A2CeAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Doc2Vec(vector_size=300, workers=8, epochs=10)\n",
        "model.build_vocab(tc1)\n",
        "model.train(tc1, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt0nWmrnC9Bl",
        "outputId": "d9832d34-e2d8-4f51-9129-a3a7bd7d3f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 223.5/223.5MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"dtv_semeval\")\n"
      ],
      "metadata": {
        "id": "x83LyUF3fZqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc3 = read_fakenews()\n",
        "model.build_vocab(tc3, update=True)\n",
        "model.train(tc3, total_examples=len(list(tc3)), epochs=model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbQcpfClkgvQ",
        "outputId": "660daed8-36b2-4f07-fcd8-19532d409acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 19.2/19.2MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"dtv_semeval_fn\")\n"
      ],
      "metadata": {
        "id": "tECSdZyWkk3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model performance"
      ],
      "metadata": {
        "id": "YkcT33-OmNMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.n_similarity(alt2, text2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkrS36T4kng_",
        "outputId": "06d764d8-ca4d-4cce-9f39-324eabe662b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9592219"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case 2\n",
        "(What is statistics?)"
      ],
      "metadata": {
        "id": "F8vbF_tZmSzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"numbers that have been collected in order to provide information about something.\"\"\"\n",
        "alt = \"\"\" the science of collecting and studying these numbers.\"\"\"\n",
        "text2 = word_tokenize(text.lower().translate(remove_punctuation_map))\n",
        "text2 = [i for i in text2 if i not in stopw]\n",
        "alt2 = word_tokenize(alt.lower().translate(remove_punctuation_map))\n",
        "alt2 = [i for i in alt2 if i not in stopw]"
      ],
      "metadata": {
        "id": "hm9KvmHvkn19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.n_similarity(alt2, text2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAN2qG4CmXHE",
        "outputId": "8748b59b-ac54-4c60-e780-a9401b1b11ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10803401"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case 3\n",
        "(What is optimization?)\n"
      ],
      "metadata": {
        "id": "aw1aTWMcnTXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Optimization is the process where we train the model repeatedly that results in a maximum and minimum function evaluation.\"\"\"\n",
        "alt = \"\"\" An optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from an allowed set and computing the value of the function.\"\"\"\n",
        "text2 = word_tokenize(text.lower().translate(remove_punctuation_map))\n",
        "text2 = [i for i in text2 if i not in stopw]\n",
        "alt2 = word_tokenize(alt.lower().translate(remove_punctuation_map))\n",
        "alt2 = [i for i in alt2 if i not in stopw]\n",
        "model.wv.n_similarity(alt2, text2)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz5TXw84nWol",
        "outputId": "087a994b-4bc3-49dd-9d7c-0ec2d5776240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.28521207"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case 4\n",
        "(What is programming language?)"
      ],
      "metadata": {
        "id": "NHZHd-6Rn3c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"A programming language is a computer language that is used by programmers (developers) to communicate with computers\"\"\"\n",
        "alt = \"\"\" A programming language is a vocabulary and set of grammatical rules for instructing a computer or computing device to perform specific tasks.\"\"\"\n",
        "text2 = word_tokenize(text.lower().translate(remove_punctuation_map))\n",
        "text2 = [i for i in text2 if i not in stopw]\n",
        "alt2 = word_tokenize(alt.lower().translate(remove_punctuation_map))\n",
        "alt2 = [i for i in alt2 if i not in stopw]\n",
        "model.wv.n_similarity(alt2, text2)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NkRZlqxns9v",
        "outputId": "7adf8282-cf88-48d5-b6d4-d4e7fbaa9018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3567021"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case 5\n",
        "(What is the mean of data?)"
      ],
      "metadata": {
        "id": "9CDl92X7ok_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"The mean (average) of a data set is found by adding all numbers in the data set and then dividing by the number of values in the set.\"\"\"\n",
        "alt = \"\"\"In statistics, the mean for a given set of observations is equal to the sum of all the values of a collection of data divided by the total number of values in the set.\"\"\"\n",
        "text2 = word_tokenize(text.lower().translate(remove_punctuation_map))\n",
        "text2 = [i for i in text2 if i not in stopw]\n",
        "alt2 = word_tokenize(alt.lower().translate(remove_punctuation_map))\n",
        "alt2 = [i for i in alt2 if i not in stopw]\n",
        "model.wv.n_similarity(alt2, text2)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTPj59xfonR9",
        "outputId": "5d407d23-714c-410c-876d-57636ce74b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.63534915"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case 6\n",
        "(What is data science?)"
      ],
      "metadata": {
        "id": "IU4zczuwo7u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data, and apply knowledge from data across a broad range of application domains.\"\"\"\n",
        "alt = \"\"\"Data science is the field of study that combines domain expertise, programming skills, and knowledge of mathematics and statistics to extract meaningful insights from data.\"\"\"\n",
        "text2 = word_tokenize(text.lower().translate(remove_punctuation_map))\n",
        "text2 = [i for i in text2 if i not in stopw]\n",
        "alt2 = word_tokenize(alt.lower().translate(remove_punctuation_map))\n",
        "alt2 = [i for i in alt2 if i not in stopw]\n",
        "model.wv.n_similarity(alt2, text2)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh0UR4nvpChO",
        "outputId": "9deff370-d11c-42da-bffd-f7f97165b237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48388988"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case 7\n",
        "(What is linear regression?)"
      ],
      "metadata": {
        "id": "LGjkuF6opahp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Linear regression analysis is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable.\"\"\"\n",
        "alt = \"\"\"Linear regression is the estimation of a continuous dependent variable or response from a list of input variables, or features.\"\"\"\n",
        "text2 = word_tokenize(text.lower().translate(remove_punctuation_map))\n",
        "text2 = [i for i in text2 if i not in stopw]\n",
        "alt2 = word_tokenize(alt.lower().translate(remove_punctuation_map))\n",
        "alt2 = [i for i in alt2 if i not in stopw]\n",
        "model.wv.n_similarity(alt2, text2)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS8U6__Gpe7M",
        "outputId": "67d159a8-a39e-4abd-933e-c71a752ac834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2637313"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case 8\n",
        "(What is neural network in AI?)"
      ],
      "metadata": {
        "id": "a4E3DCrEqSUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"A neural network is a method in artificial intelligence that teaches computers to process data in a way that is inspired by the human brain. It is a type of machine learning process, called deep learning, that uses interconnected nodes or neurons in a layered structure that resembles the human brain.\"\"\"\n",
        "alt = \"\"\"Neural networks, also known as artificial neural networks or simulated neural networks, are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\"\"\"\n",
        "text2 = word_tokenize(text.lower().translate(remove_punctuation_map))\n",
        "text2 = [i for i in text2 if i not in stopw]\n",
        "alt2 = word_tokenize(alt.lower().translate(remove_punctuation_map))\n",
        "alt2 = [i for i in alt2 if i not in stopw]\n",
        "model.wv.n_similarity(alt2, text2)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5G27NzRqgat",
        "outputId": "c1f1b507-0111-4be4-ac52-65286b8fcec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46328947"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case 9\n",
        "(What is deep learning?)"
      ],
      "metadata": {
        "id": "kab3b3OtqsBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Deep learning is a type of machine learning based on artificial neural networks in which multiple layers of processing are used to extract progressively higher level features from data.\"\"\"\n",
        "alt = \"\"\"Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning.\"\"\"\n",
        "text2 = word_tokenize(text.lower().translate(remove_punctuation_map))\n",
        "text2 = [i for i in text2 if i not in stopw]\n",
        "alt2 = word_tokenize(alt.lower().translate(remove_punctuation_map))\n",
        "alt2 = [i for i in alt2 if i not in stopw]\n",
        "model.wv.n_similarity(alt2, text2)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-Qt1LBYq8bT",
        "outputId": "0439095d-1f56-458c-8267-b53cb0884efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.56300473"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case 10\n",
        "(What is median in data science?)"
      ],
      "metadata": {
        "id": "aVt2HRcKrEE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"The median is the middle number in a sorted, ascending or descending, list of numbers.\"\"\"\n",
        "alt = \"\"\"The median is the middle value when a data set is ordered from least to greatest.\"\"\"\n",
        "text2 = word_tokenize(text.lower().translate(remove_punctuation_map))\n",
        "text2 = [i for i in text2 if i not in stopw]\n",
        "alt2 = word_tokenize(alt.lower().translate(remove_punctuation_map))\n",
        "alt2 = [i for i in alt2 if i not in stopw]\n",
        "model.wv.n_similarity(alt2, text2)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1KnrXC7rJZr",
        "outputId": "904bc6f4-8973-417f-992f-18edfd6c1e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.31520677"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}